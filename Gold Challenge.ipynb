{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bda84f38-75c1-4a63-929d-761d2c6a85ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import necesary library\n",
    "import re\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "from flask import Flask, jsonify, request\n",
    "from flasgger import Swagger, swag_from, LazyString, LazyJSONEncoder \n",
    "from flasgger import swag_from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4630bb27-9be5-4d3e-b7e6-12b524508446",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#default flask and swagger setting\n",
    "app = Flask(__name__)\n",
    "app.json_encoder = LazyJSONEncoder\n",
    "swagger_template = dict(\n",
    "    info = {\n",
    "        'title': LazyString(lambda: 'API Documentation for Data Cleansing'),\n",
    "        'version': LazyString(lambda: '1.0.0'),\n",
    "        'description': LazyString(lambda: 'API Documentation for Data Cleansing'),\n",
    "    },\n",
    "    host = LazyString(lambda: request.host)\n",
    ")\n",
    "\n",
    "swagger_config = {\n",
    "    \"headers\": [],\n",
    "    \"specs\": [\n",
    "        {\n",
    "            \"endpoint\": 'docs',\n",
    "            \"route\": '/docs.json',\n",
    "        }\n",
    "    ],\n",
    "    \"static_url_path\": \"/flasgger_static\",\n",
    "    \"swagger_ui\": True,\n",
    "    \"specs_route\": \"/docs/\"\n",
    "}\n",
    "swagger = Swagger(app, template=swagger_template, config=swagger_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db7089c-0b8c-413c-8fcd-d00450ebf47e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__' (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [05/Dec/2023 00:10:23] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [05/Dec/2023 00:10:26] \"GET /docs/ HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [05/Dec/2023 00:10:26] \"GET /flasgger_static/swagger-ui-bundle.js HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [05/Dec/2023 00:10:26] \"GET /flasgger_static/swagger-ui.css HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [05/Dec/2023 00:10:26] \"GET /flasgger_static/swagger-ui-standalone-preset.js HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [05/Dec/2023 00:10:26] \"GET /flasgger_static/lib/jquery.min.js HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [05/Dec/2023 00:10:26] \"GET /docs.json HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received file: data.csv\n",
      "DataFrame loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [05/Dec/2023 00:10:37] \"POST /file-processing HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "#define endpoints: get methods\n",
    "@swag_from(\"/Users/feybearsella_m/Documents/Binar Challenge Wave 15/hello_world.yml\", methods=['GET'])\n",
    "@app.route('/', methods=['GET'])\n",
    "def hello_world():\n",
    "    json_response = {\n",
    "        'status_code': 200,\n",
    "        'description': \"Welcome to API data cleansing\",\n",
    "        'data': \"Welcome to API data cleansing\",  \n",
    "     }\n",
    "    response_data = jsonify(json_response)\n",
    "    return response_data\n",
    "\n",
    "#define endpoints: post method for text processing\n",
    "@swag_from(\"/Users/feybearsella_m/Documents/Binar Challenge Wave 15/text_processing.yml\", methods=['POST'])\n",
    "@app.route('/text-processing', methods=['POST'])\n",
    "def text_processing():\n",
    "    \n",
    "    text = request.form.get('text')\n",
    "    \n",
    "    #to read abusive.csv \n",
    "    with open('abusive.csv', 'r', encoding='latin-1') as f:\n",
    "        abusive_dict = {line.strip():'' for line in f}\n",
    "    \n",
    "    #to read new_kamusalay\n",
    "    with open('new_kamusalay.csv', 'r', encoding='latin-1') as f:\n",
    "        alay_dict = {}\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            split_line = line.split(':')\n",
    "            if len(split_line) != 2:\n",
    "                continue\n",
    "            alay_dict[split_line[0]] = split_line[1]\n",
    "    \n",
    "    #function to process the text\n",
    "    #lowercase text\n",
    "    text = text.lower()\n",
    "    #remove URL\n",
    "    text = re.sub(r'http\\S+', '', text)   \n",
    "    #remove RT\n",
    "    text = re.sub(r'RT', '', text)\n",
    "    #remove trailing and leading whitespace\n",
    "    text = text.strip()\n",
    "    #remove multiple whitespace\n",
    "    text = re.sub(r'\\\\n',' ', text)\n",
    "    #remove new line\n",
    "    text = re.sub(r'\\n', ' ',text)\n",
    "    #remove space\n",
    "    text = re.sub('  +', ' ',text)\n",
    "    #remove punctuation\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    #remove mention\n",
    "    text = re.sub(r'@\\S+', '', text)\n",
    "    #remove emoji\n",
    "    text = re.sub(r'[\\\\x]+[a-z0-9]{2}', '', text)\n",
    "    #remove hashtag\n",
    "    text = re.sub(r'#([^\\s]+)', '', text)\n",
    "    #remove numeric number\n",
    "    text = re.sub(r'\\d+', '', text) \n",
    "    #remove word user\n",
    "    text = text.replace('user', '')\n",
    "    \n",
    "    words = text.split()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        #ignore abusive words\n",
    "        if word in abusive_dict:\n",
    "            continue\n",
    "        #replace the words\n",
    "        if word in alay_dict:\n",
    "            new_words.append(alay_dict[word])\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    \n",
    "    #cleansing text\n",
    "    new_text = ' '.join(new_words)\n",
    "  \n",
    "    return jsonify({'cleaned_text': new_text})\n",
    "\n",
    "\n",
    "#define endpoints: post method for file processing \n",
    "@swag_from(\"/Users/feybearsella_m/Documents/Binar Challenge Wave 15/file_processing.yml\", methods=['POST'])\n",
    "@app.route('/file-processing', methods=['POST'])\n",
    "def file_processing():\n",
    "    \n",
    "    #import csv file to pandas dataframe \n",
    "    df_abusive = pd.read_csv('abusive.csv', encoding='latin-1', header=None)\n",
    "    df_kamusalay = pd.read_csv('new_kamusalay.csv', encoding='latin-1', header=None)\n",
    "    \n",
    "    #get file from upload to dataframe\n",
    "    file= request.files['file']\n",
    "    print(f\"Received file: {file.filename}\")\n",
    "    \n",
    "    #import file object to pandas dataframe\n",
    "    df = pd.read_csv(file, encoding='latin-1')\n",
    "    print(\"DataFrame loaded successfully.\")\n",
    "    \n",
    "    #set tweet column for dataframe\n",
    "    df = df[['Tweet']]\n",
    "    \n",
    "    #function to drop duplicates \n",
    "    df.drop_duplicates(inplace=True)\n",
    "    \n",
    "    #function for new number of characters\n",
    "    df['no_char'] = df['Tweet'].apply(len)\n",
    "    \n",
    "    #function for new number of words\n",
    "    df['no_words'] = df['Tweet'].apply(lambda x: len(x.split()))\n",
    "    \n",
    "    #function to clean data \n",
    "    def tweet_cleansing(x):\n",
    "        tweet = x\n",
    "        #to remove non-alphabetic characters (excluding spaces) from string values in the Dataframe\n",
    "        cleaned_tweet = re.sub('[^a-zA-Z\\s]', '', tweet).strip()\n",
    "        #to lowercase all string values in the Dataframe\n",
    "        cleaned_tweet = cleaned_tweet.lower()\n",
    "        #to remove words with three or fewer characters from string values in the Dataframe\n",
    "        cleaned_tweet = re.sub(r'\\b\\w{1,3}\\b', '', tweet).strip()\n",
    "        #to remove words with 15 or more characters from string values in the Dataframe\n",
    "        cleaned_tweet = re.sub(r'\\b\\w{15,}\\b', '', tweet).strip()\n",
    "        #to remove trailing and leading whitespace in string values of the Dataframe\n",
    "        cleaned_tweet = cleaned_tweet.strip()\n",
    "        #to remove the whitespace in string values of the Dataframe\n",
    "        cleaned_tweet = re.sub(r'\\s+', ' ', tweet).strip()\n",
    "        #to remove emoji from string values in the Dataframe\n",
    "        cleaned_tweet = re.sub(r'[\\\\x]+[a-z0-9]{2}', '', tweet).strip()\n",
    "        #to remove hashtag from string values in the Dataframe\n",
    "        cleaned_tweet = re.sub(r'#([^\\s]+)', '', tweet).strip()\n",
    "        #to replace user mentions (e.g., @username) with an empty string\n",
    "        cleaned_tweet = re.sub(r'@\\w+', '', tweet).strip()\n",
    "\n",
    "        return cleaned_tweet\n",
    "    \n",
    "    #create new cleaned_tweet column\n",
    "    df['cleaned_tweet'] = df['Tweet'].apply(tweet_cleansing)\n",
    "    \n",
    "    #ensure 'cleaned_tweet' column exists\n",
    "    if 'cleaned_tweet' in df.columns:\n",
    "        df['cleaned_tweet'] = df['cleaned_tweet'].astype(str)\n",
    "    \n",
    "    #create new no_word and no_char on cleaned_tweet column\n",
    "    df['no_char_2'] = df['cleaned_tweet'].apply(len)\n",
    "    df['no_words_2'] = df['cleaned_tweet'].apply(lambda x: len(str(x).split()))\n",
    "    \n",
    "    #function to remove abousive words\n",
    "    def remove_abusive(cleaned_tweet, abusive_words):\n",
    "        #convert cleaned_tweet to lowercase\n",
    "        cleaned_tweet = cleaned_tweet.lower()\n",
    "        #remove the abusive words\n",
    "        for word in abusive_words:\n",
    "            cleaned_tweet = re.sub('[^a-zA-Z\\s]', '', cleaned_tweet).strip() #to remove non-alphabetic characters (excluding spaces)\n",
    "            cleaned_tweet = cleaned_tweet.replace(word.lower(), '').strip() #to lowercase all string\n",
    "            cleaned_tweet = re.sub(r'\\b\\w{1,3}\\b', '', cleaned_tweet).strip() #to remove words with three or fewer characters\n",
    "            cleaned_tweet = re.sub(r'\\b\\w{15,}\\b', '', cleaned_tweet).strip() #to remove words with 15 or more characters\n",
    "            cleaned_tweet = cleaned_tweet.strip() #to remove trailing and leading whitespace\n",
    "            cleaned_tweet = re.sub(r'\\s+', ' ', cleaned_tweet).strip() #to remove the whitespace in string\n",
    "            cleaned_tweet = re.sub(r'\\\\[a-z0-9]{1,5}', '', cleaned_tweet).strip() # #to remove emoji\n",
    "            cleaned_tweet = re.sub(r'@\\w+', '', cleaned_tweet) #to replace user mentions (e.g., @username) with an empty string\n",
    "            cleaned_tweet = re.sub(r'#([^\\s]+)', '', cleaned_tweet).strip()  #to remove hashtag\n",
    "             \n",
    "        #split the cleaned tweet into words\n",
    "        words = cleaned_tweet.split()\n",
    "    \n",
    "        #keep track of encountered words\n",
    "        unique_words = set()\n",
    "    \n",
    "        #remove duplicate words and join back into a string\n",
    "        cleaned_tweet = ' '.join(word for word in words if word not in unique_words and (unique_words.add(word) or True))\n",
    "    \n",
    "        return cleaned_tweet\n",
    "    \n",
    "    #function to count number of abusive words\n",
    "    def count_abusive(x, df_abusive):\n",
    "        cleaned_tweet = x\n",
    "        matched_list = []\n",
    "        for i in range(len(df_abusive)):\n",
    "            for j in x.split():\n",
    "                word = df_abusive['abusive'].iloc[i]\n",
    "                if word == j.lower():\n",
    "                    matched_list.append(word)\n",
    "        return len(matched_list)\n",
    "     \n",
    "    #assuming df_abusive is a DataFrame containing abusive words\n",
    "    df_abusive = pd.DataFrame({'abusive': ['alay', 'ampas', 'buta']})\n",
    "    \n",
    "    #remove abusive words\n",
    "    df['cleaned_tweet'] = df.apply(lambda row: remove_abusive(row['cleaned_tweet'], df_abusive['abusive']), axis=1)\n",
    "\n",
    "    #function to count abusive words and create new column\n",
    "    df['estimated_no_abs_words'] = df['cleaned_tweet'].apply(lambda x: count_abusive(x, df_abusive))\n",
    "    \n",
    "    #to save the cleaned data to csv file\n",
    "    df.to_csv('cleaned_data.csv', index=False)\n",
    "    \n",
    "    #connect and create new database\n",
    "    database_path = '/Users/feybearsella_m/Documents/Binar Challenge Wave 15/data.db'\n",
    "    with sqlite3.connect(database_path) as conn:\n",
    "        q_create_table = \"\"\"\n",
    "        create table if not exists df (Tweet varchar(255), no_char int, no_words int, cleaned_tweet varchar(255), no_char_2 int, no_words_2 int);\n",
    "        \"\"\"\n",
    "        conn.execute(q_create_table)\n",
    "        conn.commit()\n",
    "\n",
    "    #check table has data or not\n",
    "        cursor = conn.execute(\"select count(*) from df\")\n",
    "        num_rows = cursor.fetchall()\n",
    "        num_rows = num_rows[0][0]\n",
    "\n",
    "    #insert the data if table has no data   \n",
    "        if num_rows == 0:\n",
    "        # DO ITERATIONS TO INSERT DATA (EACH ROW) FROM FINAL DATAFRAME (DF)\n",
    "            for i in range(len(df)):\n",
    "                tweet = df['Tweet'].iloc[i]\n",
    "                no_char = int(df['no_char'].iloc[i])\n",
    "                no_words = int(df['no_words'].iloc[i])\n",
    "                cleaned_tweet = df['cleaned_tweet'].iloc[i]\n",
    "                no_char_2 = int(df['no_char_2'].iloc[i])\n",
    "                no_words_2 = int(df['no_words_2'].iloc[i])\n",
    "    \n",
    "                q_insertion = \"insert into df (Tweet, no_char, no_words, cleaned_tweet, no_char_2, no_words_2) values (?,?,?,?,?,?)\"\n",
    "                conn.execute(q_insertion, (tweet, no_char, no_words, cleaned_tweet, no_char_2, no_words_2))\n",
    "   \n",
    "    \n",
    "    conn.close()    \n",
    "    \n",
    "    json_response = {\n",
    "        'status_code' : 200,\n",
    "        'description' : \"File has been cleaned and saved in database and csv file.\",\n",
    "        'data' : \"cleaned_data\",\n",
    "    }\n",
    "\n",
    "    response_data = jsonify(json_response)\n",
    "    return response_data\n",
    "                     \n",
    "if __name__ == \"__main__\":\n",
    "    app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a63e479-3b48-46e3-afad-83a27f94b241",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd18a30-9417-4ca2-8616-e1ff48e7f9cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
